<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation">
  <meta name="keywords" content="LLM, CLIP">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <!-- <script src="./static/js/index.js"></script> -->
</head>
<body>

<!--<nav class="navbar" role="navigation" aria-label="main navigation">-->
<!--  <div class="navbar-brand">-->
<!--    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">-->
<!--      <span aria-hidden="true"></span>-->
<!--      <span aria-hidden="true"></span>-->
<!--      <span aria-hidden="true"></span>-->
<!--    </a>-->
<!--  </div>-->
<!--  <div class="navbar-menu">-->
<!--    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">-->
<!--      <a class="navbar-item" href="">-->
<!--      <span class="icon">-->
<!--          <i class="fas fa-home"></i>-->
<!--      </span>-->
<!--      </a>-->

<!--      <div class="navbar-item has-dropdown is-hoverable">-->
<!--        <a class="navbar-link">-->
<!--          More Research-->
<!--        </a>-->
<!--        &lt;!&ndash; <div class="navbar-dropdown">-->
<!--          <a class="navbar-item" href="https://hypernerf.github.io">-->
<!--            HyperNeRF-->
<!--          </a>-->
<!--          <a class="navbar-item" href="https://nerfies.github.io">-->
<!--            Nerfies-->
<!--          </a>-->
<!--          <a class="navbar-item" href="https://latentfusion.github.io">-->
<!--            LatentFusion-->
<!--          </a>-->
<!--          <a class="navbar-item" href="https://photoshape.github.io">-->
<!--            PhotoShape-->
<!--          </a>-->
<!--        </div> &ndash;&gt;-->
<!--      </div>-->
<!--    </div>-->

<!--  </div>-->
<!--</nav>-->


<section class="hero">
  <div class="hero-body"></div>
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Weiquan Huang<sup>1*</sup>,</span>
            <span class="author-block">
              Aoqi Wu<sup>1*</sup>,</span>
            <span class="author-block">
              Yifan Yang<sup>2†</sup>,
            </span>
            <span class="author-block">
              Xufang Luo<sup>2</sup>,
            </span>
            <span class="author-block">
              Yuqing Yang<sup>2</sup>,
            </span>
            <span class="author-block">
              Liang Hu<sup>1</sup>,
            </span>
            <span class="author-block">
              Qi Dai<sup>2</sup>,
            </span>
            <span class="author-block">
              Xiyang Dai<sup>2</sup>,
            </span>
            <span class="author-block">
              Dongdong Chen<sup>2</sup>,
            </span>
            <span class="author-block">
              Chong Luo<sup>2</sup>,
            </span>
            <span class="author-block">
              Lili Qiu<sup>2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Tongji University,</span>
            <span class="author-block"><sup>2</sup>Microsoft Corporation</span>
          </div>
<!--          <div class="is-size-5 publication-authors">-->
<!--            <span class="author-block"></span>-->
<!--        </div>-->
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>*</sup>Equal Contribution, work done during internship at Microsoft Research Asia</span>
        </div>
        <div class="is-size-5 publication-authors">
          <span class="author-block"><sup>†</sup>Corresponding to:
              <a class="pl-2" href="mailto:yifanyang@microsoft.com">yifanyang@microsoft.com</a>

          </span>

      </div>
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/microsoft/LLM2CLIP/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Huggingface Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/collections/microsoft/llm2clip-672323a266173cfa40b32d4c"
                   class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <!-- <i class="fab fa-huggingface"></i> -->
                       <img src="static/images/huggingface_logo-noborder.svg" alt="">
                    </span>
                  <span>HF</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" style="font-family: sans-serif;">
  <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
          <div class="column">
              <h3 class="title is-3" style="padding: 50px 0 0 0;">Abstract</h3>
              <div class="content has-text-justified" >
                  <p style="font-size: 100%">
                    CLIP is one of the most important multimodal foundational models today, aligning visual and
                    textual signals into a shared feature space using a simple contrastive learning loss on large-scale
                    image-text pairs. What powers CLIP’s capabilities? The rich supervision signals provided by
                    natural language — the carrier of human knowledge — shape a powerful cross-modal represen-
                    tation space. As a result, CLIP supports a variety of tasks, including zero-shot classification,
                    detection, segmentation, and cross-modal retrieval, significantly influencing the entire multi-
                    modal domain. However, with the rapid advancements in large language models (LLMs) like
                    GPT-4 and LLaMA, the boundaries of language comprehension and generation are continually
                    being pushed. This raises an intriguing question: can the capabilities of LLMs be harnessed
                    to further improve multimodal representation learning? The potential benefits of incorporating
                    LLMs into CLIP are clear. LLMs’ strong textual understanding can fundamentally improve
                    CLIP’s ability to handle image captions, drastically enhancing its ability to process long and
                    complex texts — a well-known limitation of vanilla CLIP. Moreover, LLMs are trained on a
                    vast corpus of text, possessing open-world knowledge. This allows them to expand on cap-
                    tion information during training, increasing the efficiency of the learning process. However,
                    realizing this potential is challenging. Despite LLMs’ powerful internal comprehension, their
                    autoregressive nature hides this capability within the model, leading to output features with
                    poor discriminability. Our experiments show that directly integrating LLMs into CLIP results
                    in catastrophic performance drops. In this paper, we propose LLM2CLIP, a novel approach that
                    embraces the power of LLMs to unlock CLIP’s potential. By fine-tuning the LLM in the caption
                    space with contrastive learning, we extract its textual capabilities into the output embeddings,
                    significantly improving the output layer’s textual discriminability. We then design an efficient
                    training process where the fine-tuned LLM acts as a powerful teacher for CLIP’s visual encoder.
                    Thanks to the LLM’s presence, we can now incorporate longer and more complex captions
                    without being restricted by vanilla CLIP text encoder’s context window and ability limitations.
                    Our experiments demonstrate that this approach brings substantial improvements in cross-modal
                    tasks. Our method directly boosted the performance of the previously SOTA EVA02 model by
                    16.5% on both long-text and short-text retrieval tasks, transforming a CLIP model trained solely
                    on English data into a state-of-the-art cross-lingual model. Moreover, when integrated into mul-
                    timodal training with models like Llava 1.5, it consistently outperformed CLIP across nearly all
                    benchmarks, demonstrating comprehensive performance improvements.
                  </p>
      </div>
  </div>
</section>

<div class="container is-max-desktop">
  <div class="container is-max-widescreen">
      <div class="rows">
          <div class="rows is-centered ">
              <div class="row is-full-width">
                <img src="static/images/main.svg" alt="COCO Score">
                  <span ><b>LLM2CLIP Overview.</b>  After applying caption contrastive fine-tuning to the LLM, the increased textual
                    discriminability enables more effective CLIP training. We leverage the open-world knowledge and general capa-
                    bilities of the LLM to better process dense captions, addressing the previous limitations of the pretrained CLIP
                    visual encoder and providing richer, higher-dimensional textual supervision. Experimental results demonstrate
                    that LLM2CLIP can make any SOTA CLIP model even more SOTA ever</span>
              </div>
          </div>
      </div>
  </div>
</div>
<div class="container is-max-desktop">
  <div class="container is-max-widescreen">
      <div class="rows">
          <div class="rows is-centered ">
              <div class="row is-full-width">
              <div style="text-align: center;">
                  <img src="static/images/custom_radar_chart_no_outer_grid.svg" alt="radar figure" style="width: 60%;">

              </div>
              <div style="text-align: center;">
                <span ><b>LLM2CLIP can make SOTA CLIP even more SOTA ever.</b> </span>


            </div>
              </div>
          </div>
      </div>
  </div>
</div>




<!--<section class="section">-->
<!--  <div class="container is-max-desktop">-->
<!--    <div class="container is-max-widescreen">-->
<!--      <div class="rows">-->
<!--          <div class="rows is-centered ">-->
<!--              <div class="row is-full-width">-->
<!--                  <h3 class="title is-3"><span class="dvima">Insights</span></h3>-->
<!--                  <ol>-->
<!--                      <li><p>Natural language is redundant, amount of information varies.</p></li>-->
<!--                      <li><p>LLMs can understand compressed prompt.</p></li>-->
<!--                      <li><p>There is a trade-off between language completeness and compression ratio. <b>(LLMLingua)</b></p></li>-->
<!--                      <li><p>GPT-4 can recover all the key information from a compressed prompt-emergent ability. <b>(LLMLingua)</b></p></li>-->
<!--                  </ol>-->
<!--                  <br/>-->
<!--                  <p>For more details, please refer to the paper <a href=""><b>LLM2CLIP</b></a>.</p>-->
<!--              </div>-->
<!--          </div>-->
<!--      </div>-->
<!--      <br>-->
<!--  </div>-->
<!--  -->
<!--      <div class="container is-max-widescreen">-->
<!--        <div class="rows">-->
<!--            <div class="rows is-centered ">-->
<!--                <div class="row is-full-width">-->
<!--                    <h2 class="title is-3"><span class="dvima">Why <i>LLM2CLIP</i>?</span></h2>-->
<!--                    <p style="font-size: 100%">-->
<!--                        -->
<!--                        LLM2CLIP offers the following advantages:<br/>-->
<!--                        <ol>-->
<!--                          <li><b>Improved Long-Text Handling</b>: LLMs enhance CLIP’s ability to process long and complex captions, addressing a known limitation of vanilla CLIP.</li>-->
<!--                          <li><b>Open-World Knowledge</b>: LLMs, trained on vast text corpora, bring open-world knowledge that can enrich image captions with additional context.</li>-->
<!--                          <li><b>Increased Learning Efficiency</b>: Leveraging LLMs allows CLIP to learn more effectively from enriched textual information, speeding up the training process.</li>-->
<!--                      </ol>                      -->
<!--                    </p>-->
<!--                </div>-->
<!--            </div>-->
<!--        </div>-->
<!--    </div>-->
    <div class="container is-max-widescreen">
      <div class="rows">
          <div class="rows is-centered ">
              <div class="row is-full-width">
                  <h2 class="title is-3"><span class="dvima">What challenges?</span></h2>
                  <p style="font-size: 100%">
                    In the cross-modal contrastive learning framework employed by CLIP, the text encoder
                    functions as a set of knowledge anchors in the shared latent space, guiding the alignment of the vision encoder with
                    human knowledge of the physical world. The structure, richness, and discriminability of these knowledge anchors
                    are critical to the visual model’s effectiveness. In contrast, LLMs are primarily designed to predict the next word
                    rather than generate explicit representations of the knowledge they contain. Their textual comprehension abilities
                    and open-world knowledge are latent within the model, rather than present in the output embeddings, making them
                    difficult to utilize in the same explicit manner as CLIP’s text encoder. As a result, using LLMs as a text encoder
                    may not produce linearly separable features, which are crucial for effective feature alignment
                    <br>
                    <br>
                    To validate our hypothesis, we designed a caption-to-caption retrieval experiment, as shown in Table 1 and Fig-
ure 2. Each image in the MS-COCO dataset has five human-annotated captions. We selected the first two captions
as positive samples and performed retrieval across the entire validation set. Using the caption retrieval accuracy
(CRA), we evaluated the text model’s ability to differentiate between captions, helping us determine which lan-
guage model is better suited for CLIP. We found that Llama-3 8B achieved only 18.4% top-1 accuracy, while
the standard CLIP-ViT-L reached 66.0% top-1 accuracy. As illustrated in Figure 2, the top-1 caption retrieved
by original Llama-3 can be entirely unrelated to the query caption, clearly obstructing effective CLIP learning.
Therefore, directly using an LLM to guide CLIP’s visual encoder training is highly constrained.                     
                  </p>
                  <div style="display: flex; align-items: center; justify-content: space-between;">
                    <div style="width: 30%;">
                        <table border="1" cellspacing="0" cellpadding="5" style="width: 100%; text-align: center;">
                            <caption><b>Comparison of top-1 Caption Retrieval Accuracy (CRA) for various language models in MS COCO validation set.</b></caption>
                            <thead>
                                <tr>
                                    <th><b>Language Model</b></th>
                                    <th><b>CRA</b></th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>CLIP-L/14</td>
                                    <td>66.6</td>
                                </tr>
                                <tr>
                                    <td>EVA02-L/14</td>
                                    <td>69.8</td>
                                </tr>
                                <tr style="background-color: #f8d7da;">
                                    <td style="color: red;">Llama3-8B</td>
                                    <td style="color: red;">18.4</td>
                                </tr>
                                <tr style="background-color: #f8d7da;">
                                    <td style="color: red;">Llama3.2-1B</td>
                                    <td style="color: red;">18.3</td>
                                </tr>
                                <tr>
                                    <td>Llama3-8B-CC</td>
                                    <td><b>73.0</b></td>
                                </tr>
                                <tr>
                                    <td>Llama3.2-1B-CC</td>
                                    <td>72.8</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                    <div style="width: 65%; text-align: center;">
                        <img src="static/images/coco_score.svg" alt="COCO Score">
                    </div>
                </div>
                
              </div>
          </div>
      </div>
  </div>

    <div class="container is-max-widescreen">
      <div class="rows">
          <div class="rows is-centered ">
              <div class="row is-full-width">
                  <h2 class="title is-3"><span class="dvima">LLM2CLIP can make SOTA CLIP even more SOTA ever. </span></h2>
                  <div class="content has-text-justified">
                    <table border="1" cellspacing="0" cellpadding="5" style="width: 100%; text-align: center;">
                      <caption>Comparison of various text encoders. With only a few epochs of training alongside the LLM, we can significantly enhance the performance of existing pretrained SOTA CLIP models. We freeze the gradients of the LLM and only use it for feature extraction, making the training process very low-cost.</caption>
                      <thead>
                          <tr>
                              <th rowspan="2">Methods</th>
                              <th colspan="2">Flickr30k</th>
                              <th colspan="2">COCO</th>
                              <th colspan="2">ShareGPT4v</th>
                              <th colspan="2">Urban-1k</th>
                              <th colspan="2">DOCCI</th>
                              <th rowspan="2">Average</th>
                              <th rowspan="2">CRA</th>
                          </tr>
                          <tr>
                              <th>I2T</th>
                              <th>T2I</th>
                              <th>I2T</th>
                              <th>T2I</th>
                              <th>I2T</th>
                              <th>T2I</th>
                              <th>I2T</th>
                              <th>T2I</th>
                              <th>I2T</th>
                              <th>T2I</th>
                          </tr>
                      </thead>
                      <tbody>
                          <tr>
                              <td><strong>EVA02 Vit-L/14</strong></td>
                              <td>89.8</td>
                              <td>73.3</td>
                              <td>63.8</td>
                              <td>63.8</td>
                              <td>89.3</td>
                              <td>91.9</td>
                              <td>68.5</td>
                              <td>73.3</td>
                              <td>75.0</td>
                              <td>73.4</td>
                              <td>76.2</td>
                              <td>69.8</td>
                          </tr>
                          <tr>
                              <td>&#8195;<strong>+ Jina Bert</strong></td>
                              <td>87.9</td>
                              <td>77.9</td>
                              <td>60.9</td>
                              <td>50.3</td>
                              <td>95.3</td>
                              <td>95.1</td>
                              <td>79.4</td>
                              <td>83.8</td>
                              <td>73.8</td>
                              <td>77.9</td>
                              <td>78.2</td>
                              <td><strong>74.2</strong></td>
                          </tr>
                          <tr>
                              <td>&#8195;<strong>+ Llama3-8B</strong></td>
                              <td>87.1</td>
                              <td>75.3</td>
                              <td>56.4</td>
                              <td>41.6</td>
                              <td>89.3</td>
                              <td>91.4</td>
                              <td>58.6</td>
                              <td>60.9</td>
                              <td>51.7</td>
                              <td>50.6</td>
                              <td>66.3</td>
                              <td>18.4</td>
                          </tr>
                          <tr>
                              <td>&#8195;<strong>+ Llama3-8B-TC</strong></td>
                              <td>92.7</td>
                              <td>82.1</td>
                              <td>68.1</td>
                              <td>54.6</td>
                              <td>97.7</td>
                              <td>98.2</td>
                              <td>88.9</td>
                              <td>93.8</td>
                              <td>85.0</td>
                              <td>87.8</td>
                              <td>84.8</td>
                              <td>71.3</td>
                          </tr>
                          <tr>
                              <td>&#8195;<strong>+ Llama3-8B-CC</strong></td>
                              <td>92.0</td>
                              <td>82.8</td>
                              <td><strong>68.5</strong></td>
                              <td><strong>54.8</strong></td>
                              <td><strong>98.6</strong></td>
                              <td><strong>99.0</strong></td>
                              <td>88.1</td>
                              <td>94.0</td>
                              <td><strong>88.2</strong></td>
                              <td><strong>90.4</strong></td>
                              <td>85.6</td>
                              <td>73.0</td>
                          </tr>
                          <tr>
                              <td>&#8195;<strong>+ Llama3.2-1B-CC</strong></td>
                              <td>91.6</td>
                              <td>81.3</td>
                              <td>65.8</td>
                              <td>52.5</td>
                              <td>98.3</td>
                              <td>98.2</td>
                              <td>84.5</td>
                              <td>91.9</td>
                              <td>83.4</td>
                              <td>86.4</td>
                              <td>83.4</td>
                              <td>72.8</td>
                          </tr>
                          <tr>
                              <td>&#8195;<strong>+ Mistral-Nemo-12B-CC</strong></td>
                              <td><strong>93.5</strong></td>
                              <td><strong>83.7</strong></td>
                              <td><strong>68.5</strong></td>
                              <td>54.7</td>
                              <td><strong>98.6</strong></td>
                              <td>98.9</td>
                              <td><strong>90.4</strong></td>
                              <td><strong>94.3</strong></td>
                              <td>88.0</td>
                              <td>89.7</td>
                              <td><strong>86.0</strong></td>
                              <td>73.3</td>
                          </tr>
                      </tbody>
                  </table>
                  
                  </div>
              </div>
              <div class="row is-full-width">
                <h2 class="title is-3"><span class="dvima">LLM2CLIP enables CLIP's cross-language retrieval performance </span></h2>
                <!-- <p style="font-size: 100%">
                    We tested LLMLingua in various scenarios, including reasoning, in-context learning (ICL), summarization, and dialogue. The results in the GSM8K and Big-bench Hard Benchmark are listed below. Notably, within the GSM8K, LLMLingua was able to retain the reasoning capabilities of LLMs at a 20x compression ratio, with only a 1.5% loss in performance. For more detailed experimental results, please refer to the paper.<br/>
                </p> -->
                <div class="content has-text-justified">
                  <table border="1" cellspacing="0" cellpadding="5" style="width: 100%;  text-align: center; ">
                    <colgroup>
                      <col style="width: 10em;"> <!-- 第一列宽度为 15 倍字体大小 -->
                  </colgroup>
                    <caption>Retrieval Performance across Flickr30K-CN and COCO-CN. Even with alignment done solely on English data, our network enables Chinese cross-modal retrieval to go from unusable to SOTA, surpassing models like Wukong that are trained on Chinese datasets.
                    </caption>
                    <thead style="font-size:14px;">
                        <tr>
                            <th rowspan="2">Methods</th>
                            <th colspan="6">Flickr-CN</th>
                            <th colspan="6">COCO-CN</th>
                        </tr>
                        <tr>
                            <th>I2T@1</th>
                            <th>I2T@5</th>
                            <th>I2T@10</th>
                            <th>T2I@1</th>
                            <th>T2I@5</th>
                            <th>T2I@10</th>
                            <th>I2T@1</th>
                            <th>I2T@5</th>
                            <th>I2T@10</th>
                            <th>T2I@1</th>
                            <th>T2I@5</th>
                            <th>T2I@10</th>
                        </tr>
                    </thead>
                    <tbody style="font-size:14px;">
                        <tr>
                            <td><strong>ViT-L/14-336</strong></td>
                            <td></td>
                            <td></td>
                            <td></td>
                            <td></td>
                            <td></td>
                            <td></td>
                            <td></td>
                            <td></td>
                            <td></td>
                            <td></td>
                            <td></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td>Wukong</td>
                            <td>76.1</td>
                            <td>94.8</td>
                            <td>97.5</td>
                            <td>51.7</td>
                            <td>78.9</td>
                            <td>86.3</td>
                            <td>53.4</td>
                            <td>80.2</td>
                            <td>90.1</td>
                            <td>55.2</td>
                            <td>81.0</td>
                            <td>90.6</td>
                        </tr>
                        <tr>
                            <td>CN-CLIP</td>
                            <td>80.2</td>
                            <td>96.6</td>
                            <td>98.2</td>
                            <td>68.0</td>
                            <td>90.7</td>
                            <td>95.4</td>
                            <td>63.4</td>
                            <td>84.2</td>
                            <td>92.9</td>
                            <td>64.0</td>
                            <td>89.2</td>
                            <td>94.4</td>
                        </tr>
                        <tr>
                            <td>JinaCLIP</td>
                            <td>3.30</td>
                            <td>9.90</td>
                            <td>15.1</td>
                            <td>0.7</td>
                            <td>3.5</td>
                            <td>6.0</td>
                            <td>2.9</td>
                            <td>8.9</td>
                            <td>13.7</td>
                            <td>1.0</td>
                            <td>4.9</td>
                            <td>8.2</td>
                        </tr>
                        <tr>
                            <td>EVA02</td>
                            <td>4.40</td>
                            <td>11.8</td>
                            <td>16.7</td>
                            <td>0.94</td>
                            <td>2.9</td>
                            <td>4.8</td>
                            <td>2.7</td>
                            <td>9.8</td>
                            <td>15.2</td>
                            <td>1.0</td>
                            <td>3.7</td>
                            <td>7.3</td>
                        </tr>
                        <tr >
                            <td>&#8195;<strong>+ ourmethod</strong></td>
                            <td><strong>86.9</strong></td>
                            <td><strong>98.1</strong></td>
                            <td><strong>99.3</strong></td>
                            <td><strong>75.1</strong></td>
                            <td><strong>92.9</strong></td>
                            <td><strong>96.0</strong></td>
                            <td><strong>69.1</strong></td>
                            <td><strong>92.5</strong></td>
                            <td><strong>97.2</strong></td>
                            <td><strong>70.0</strong></td>
                            <td><strong>92.6</strong></td>
                            <td><strong>96.7</strong></td>
                        </tr>
                    </tbody>
                </table>
                
                </div>
            </div>
            <div class="row is-full-width">
              <h2 class="title is-3"><span class="dvima">LLM2CLIP can make models like Llava more powerful and comprehensive in performance:
              </span></h2>
              <!-- <p style="font-size: 100%">
                  We tested LLMLingua in various scenarios, including reasoning, in-context learning (ICL), summarization, and dialogue. The results in the GSM8K and Big-bench Hard Benchmark are listed below. Notably, within the GSM8K, LLMLingua was able to retain the reasoning capabilities of LLMs at a 20x compression ratio, with only a 1.5% loss in performance. For more detailed experimental results, please refer to the paper.<br/>
              </p> -->
              <div class="content has-text-justified">
                  <table border="1" cellspacing="0" cellpadding="5" style="width: 100%; text-align: center; ">
                    <colgroup>
                      <col style="width: 10em;"> <!-- 第一列宽度为 15 倍字体大小 -->
                  </colgroup>
                    <caption>Performance of Llava 1.5. We explored whether LLM2CLIP could enhance complex image understanding tasks by modifying Llava's visual encoder. LLM2CLIP improve Llava in 87.5% benchmarks. </caption>
                    <thead style="font-size:10px;">
                        <tr>
                            <th rowspan="2"><strong>MODEL</strong></th>
                            <th colspan="5"><strong>VQA Datasets</strong></th>
                            <th colspan="3"><strong>Pope Metrics</strong></th>
                            <th colspan="5"><strong>MM Benchmarks</strong></th>
                            <th colspan="3"><strong>Seed Benchmarks</strong></th>
                        </tr>
                        <tr>
                            <th><strong>VQAv2</strong></th>
                            <th><strong>GQA</strong></th>
                            <th><strong>VizWiz</strong></th>
                            <th><strong>SQA-IMG</strong></th>
                            <th><strong>TextVQA</strong></th>
                            <th><strong>Random</strong></th>
                            <th><strong>Adv.</strong></th>
                            <th><strong>Popular</strong></th>
                            <th><strong>MME</strong></th>
                            <th><strong>MMBench</strong></th>
                            <th><strong>MMBench-CN</strong></th>
                            <th><strong>LlavaBench</strong></th>
                            <th><strong>MMVet</strong></th>
                            <th><strong>All</strong></th>
                            <th><strong>IMG</strong></th>
                            <th><strong>Video</strong></th>
                        </tr>
                    </thead>
                    <tbody style="font-size:10px;">
                        <tr style="color: gray;">
                            <td><strong>Llava (Paper)</strong></td>
                            <td>78.5</td>
                            <td>62.0</td>
                            <td>50.0</td>
                            <td>66.8</td>
                            <td>58.2</td>
                            <td>87.3</td>
                            <td>86.1</td>
                            <td>84.2</td>
                            <td>1510.7</td>
                            <td>64.3</td>
                            <td>58.3</td>
                            <td>65.4</td>
                            <td>31.1</td>
                            <td>58.6</td>
                            <td>66.1</td>
                            <td>37.3</td>
                        </tr>
                        <tr>
                            <td><strong>Llava (Rep.)</strong></td>
                            <td>79.04</td>
                            <td>62.86</td>
                            <td>50.57</td>
                            <td>67.97</td>
                            <td>57.48</td>
                            <td>87.7</td>
                            <td><strong>84.85</strong></td>
                            <td>86.3</td>
                            <td>1476.69</td>
                            <td>66.66</td>
                            <td>60.39</td>
                            <td>58.0</td>
                            <td>34.3</td>
                            <td>59.86</td>
                            <td>66.95</td>
                            <td><strong>39.71</strong></td>
                        </tr>
                        <tr style="background-color: #E8F4FF;">
                            <td>&#8195;<strong>+ ourmethod</strong></td>
                            <td><strong>79.80</strong></td>
                            <td><strong>63.15</strong></td>
                            <td><strong>52.37</strong></td>
                            <td><strong>69.92</strong></td>
                            <td><strong>58.35</strong></td>
                            <td><strong>88.55</strong></td>
                            <td>82.76</td>
                            <td><strong>87.75</strong></td>
                            <td><strong>1505.82</strong></td>
                            <td><strong>68.29</strong></td>
                            <td><strong>60.40</strong></td>
                            <td><strong>62.7</strong></td>
                            <td><strong>34.8</strong></td>
                            <td><strong>60.96</strong></td>
                            <td><strong>68.80</strong></td>
                            <td>38.96</td>
                        </tr>
                    </tbody>
                </table>
                
              
              </div>
          </div>
          </div>
      </div>
  </div>


     
    <!--/ Animation. -->


   

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code></code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
      <div class="columns is-centered">
          <div class="column">
              <div class="content has-text-centered">
                  <p>
                      © 2024 Microsoft <br/>
                      Website template borrowed from <a
                          href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>,
                      <a href="https://vimalabs.github.io/">VIMA</a>
                  </p>
              </div>
          </div>
      </div>
  </div>
</footer>

</body>
</html>
